import zipfile
import os
from PIL import Image
from os import listdir
from os.path import isdir, join # Import 'join' for path construction
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG19
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import optimizers

#Extraindo o dataset
caminho_zip = '/content/Esquizofrenia-controle (1).zip'
caminho_extracao = '/content/dataset'
with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:
    zip_ref.extractall(caminho_extracao)

# Função para selecionar e processar imagem
def select_image(filename):
    image = Image.open(filename)
    image = image.convert('RGB')
    image = image.resize((300, 300))
    return np.asarray(image)

# Carregamento das imagens e labels
def load_class(diretorio, classe, imagens, labels):
    for filename in listdir(diretorio):
        path = join(diretorio, filename) # Use 'join' for correct path construction

        try:
          imagens.append(select_image(path))
          labels.append(classe)
        except:
          print("Erro ao ler imagem {}".format(path))

    return imagens, labels

# Seleção do dataset
def select_data_set(diretorio):
    imagens = list()
    labels = list()
    for subdir in listdir(diretorio):
        path = join(diretorio, subdir)  # Use 'join' for correct path construction
        if not isdir(path):
          continue
        imagens, labels = load_class(path, subdir, imagens, labels)
    return imagens, labels

# Carregar imagens e labels
dataset = '/content/dataset/Esquizofrenia-controle'
imagens, labels = select_data_set(dataset)

# Check if any images were loaded
print("Number of images loaded:", len(imagens))
print("Number of labels loaded:", len(labels))

#Normalização das imagens
imagens = np.array(imagens) / 255.0
labels = np.array(labels)

# Binarização dos labels
lb = LabelBinarizer()
labels = lb.fit_transform(labels)
labels = to_categorical(labels)

#Definição de hyparâmetros
batch_size = 2
input_shape = (150, 150, 3)
random_state = 42
alpha = 1e-5
epoch = 100

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau

filepath="transferlearning_weights.keras"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, min_delta=alpha, patience=5, verbose=1)

callbacks = [checkpoint, lr_reduce]

#Dividindo teste e treino
(trainX, testX, trainY, testY) = train_test_split(imagens, labels, test_size=0.2, stratify=labels, random_state=random_state)

#DataAugmentation
train_datagen = ImageDataGenerator(
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=False ,
    vertical_flip=False ,
    width_shift_range=0.2,
    height_shift_range=0.2,
)
train_datagen.fit(trainX)

data_aug = train_datagen.flow(trainX, trainY, batch_size=batch_size)

#TransferLearning
conv_base = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)

conv_base.trainable = True
set_trainable = False

for layer in conv_base.layers:
  if layer.name == 'block5_conv1':
    set_trainable = True
  if set_trainable:
    layer.trainable = True
  else:
    layer.trainable = False

#Criando o modelo
model = models.Sequential()
model.add(conv_base)
model.add(layers.GlobalAveragePooling2D())
model.add(layers.BatchNormalization())
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.6))
model.add(layers.Dense(2, activation='softmax'))

#Compilando o modelo
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['acc'])

#Treinando o modelo
history = model.fit(data_aug,
                    steps_per_epoch=len(trainX)//batch_size,
                    validation_data=(testX, testY),
                    callbacks=callbacks,
                    epochs=epoch)

#Gerando gráficos de acurácia e de perda
import matplotlib.pyplot as plt
%matplotlib inline

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Gerando matriz de confusão
from sklearn.metrics import confusion_matrix
pred = model.predict(testX)
pred = np.argmax(pred, axis = 1)
y_true = np.argmax(testY,axis = 1)

cm = confusion_matrix(y_true, pred)
total = sum(sum(cm))
acc = (cm[0,0] + cm[1,1]) / total
sensitivity = cm[0,0] / (cm[0,0] + cm[0,1])
specificity = cm[1,1] / (cm[1,0] + cm[1,1])

print("Acurácia {:.4f}".format(acc))
print("Sensibilidade {:.4f}".format(sensitivity))
print("Especificidade {:.4f}".format(specificity))

from mlxtend.plotting import plot_confusion_matrix
fig, ax = plot_confusion_matrix(conf_mat=cm , figsize=(5,5))
plt.show()
